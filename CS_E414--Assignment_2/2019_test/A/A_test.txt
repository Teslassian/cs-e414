1: 
	from pprint import pprint
	
	def sigmoid(x): 
		return 1/(1+np.exp(-x)) 

	w_r = w[:,np.newaxis] 
	grad_l = -((y - sigmoid(w_r.T @ X)) @ X.T).T + 1/lambda_*w_r
	pprint(grad_l)




2: 

	Non-linear classifier 
	Three classes, two features 
	Naive Bayes classifier 

 


3: 

	A : 0 
	B : 1 




4:

	Classifier constructed using Generative Models
	Linear classifier
	Full Gaussian class-conditional based classifier (shared Sigma)
	Softmax classifier
	Classifier constructed using Discriminative Models
	


	
5:
	
	x_2 = -1/3 * x_1 - 2/3




6:
	
	def sigmoid(x):
	return 1/(1+np.exp(-x))

	w = w[:,np.newaxis]
	lambda_ = 1
	N = X.shape[1]
	d = X.shape[0]
	I = np.eye(d)
	H = 0
	for n in range(N):
	H += sigmoid(w.T @ X[:,n]) * (1-sigmoid(w.T @ X[:,n])) * (X[:,n,None] @ X[:,n,None].T)
	H += 1/lambda_*I
	pprint(H)    
	


 
7:

	N = 200
	d = 4
	k = 3
	
	Naive Bayes Shared Sigma: 			kd + d = 16
	
	Naive Bayes Non Shared Sigma: 		kd + kd = 24
	
	Full Gaussian Shared Sigma: 		kd + d/2*(d+1) = 22
	
	Full Gaussian Non Shared Sigma:		kd + kd/2*(d+1) = 42
	
	
	
8:

	X = np.array([[1,2,3,4],[5,6,7,8]])
	y = np.array([1,2,1,2])

	N = X.shape[1]
	d = X.shape[0]
	C = list(set(y))
	K = len(C)
	Nj = []
	for c in C:
		Nj.append(len(y[y==c]))

	Uj = []
	Sj = []
	for c,i in zip(C, range(K)):
		Uj.append(1/Nj[i] * np.sum(X[:,y==c], axis=1)[:,np.newaxis])
		Sj.append(1/Nj[i] * np.sum((X[:,y==c]-Uj[i]) * (X[:,y==c]-Uj[i]), axis=1))
				
	pprint(Uj)
	pprint(Sj)
	
9.
	b
	e
	